import math
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import GPT2Tokenizer
from torch.cuda.amp import GradScaler, autocast
from torch.utils.checkpoint import checkpoint
import time

# ========================
# 1. æ¨¡å‹å®šä¹‰
# ========================

class TinyGPTConfig:
    def __init__(self):
        self.vocab_size = 50257  # GPT-2 tokenizer çš„è¯è¡¨å¤§å°
        self.block_size = 128    # åºåˆ—é•¿åº¦
        self.n_layer = 4
        self.n_head = 4
        self.n_embd = 256
        self.dropout = 0.1

class TinyGPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.block_size, config.n_embd)
        self.drop = nn.Dropout(config.dropout)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.wte.weight = self.lm_head.weight  # æƒé‡å…±äº«
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=0.02)
            if isinstance(module, nn.Linear) and module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        assert t <= self.config.block_size, f"è¾“å…¥é•¿åº¦ {t} è¶…è¿‡ {self.config.block_size}"

        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)
        tok_emb = self.wte(idx)
        pos_emb = self.wpe(pos)
        x = self.drop(tok_emb + pos_emb)

        for block in self.blocks:
            x = checkpoint(block, x)  # ğŸ‘ˆ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼

        x = self.ln_f(x)
        logits = self.lm_head(x)

        loss = None
        if targets is not None:
            # loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-1)

        return logits, loss

class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                     .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size()
        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)
        self.act = nn.GELU()

    def forward(self, x):
        x = self.c_fc(x)
        x = self.act(x)
        x = self.c_proj(x)
        return x

# ========================
# 2. æ•°æ®é›†ç±»
# ========================

class TextDataset(Dataset):
    def __init__(self, tokenizer, file_path, block_size=128):
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        self.examples = []
        tokenized_text = tokenizer.encode(text)

        # æ»‘åŠ¨çª—å£åˆ‡åˆ†
        for i in range(0, len(tokenized_text) - block_size + 1, block_size // 2):  # æ­¥é•¿è®¾ä¸ºä¸€åŠï¼Œå¢åŠ æ ·æœ¬
            self.examples.append(tokenized_text[i:i + block_size])

        print(f"å…±åŠ è½½ {len(self.examples)} ä¸ªè®­ç»ƒæ ·æœ¬")

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return torch.tensor(self.examples[i], dtype=torch.long)

# ========================
# 3. è®­ç»ƒä¸»å‡½æ•°
# ========================

def train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆå§‹åŒ– tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # åˆå§‹åŒ–æ¨¡å‹
    config = TinyGPTConfig()
    model = TinyGPT(config)
    model.to(device)

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params / 1e6:.2f}M")

    # åŠ è½½æ•°æ®
    dataset = TextDataset(tokenizer, "å‡¡äººutf8.txt", block_size=config.block_size)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # âš ï¸ batch_size=8 æ˜¯å®‰å…¨èµ·ç‚¹

    # ä¼˜åŒ–å™¨ & æ··åˆç²¾åº¦
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    scaler = GradScaler()  # ğŸ‘ˆ æ··åˆç²¾åº¦è®­ç»ƒæ ¸å¿ƒ

    # è®­ç»ƒå‚æ•°
    epochs = 3
    model.train()

    for epoch in range(epochs):
        total_loss = 0
        start_time = time.time()

        for step, batch in enumerate(dataloader):
            batch = batch.to(device)
            inputs, targets = batch[:, :-1], batch[:, 1:]  # è¯­è¨€æ¨¡å‹ï¼šç”¨å‰T-1ä¸ªé¢„æµ‹åT-1ä¸ª

            optimizer.zero_grad()

            # ğŸ‘‡ æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            with autocast():
                logits, loss = model(inputs, targets)

            # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

            if step % 50 == 0:
                print(f"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}")

        avg_loss = total_loss / len(dataloader)
        epoch_time = time.time() - start_time
        print(f"Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}ï¼Œè€—æ—¶: {epoch_time:.2f}ç§’")

        # æ¯ä¸ª epoch åä¿å­˜æ¨¡å‹
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
        }, f"nanoGPT2_epoch_{epoch+1}.pt")

    print("è®­ç»ƒå®Œæˆï¼")

# ========================
# 4. æ¨ç†å‡½æ•°
# ========================

def generate_text(model, tokenizer, prompt, max_new_tokens=50):
    model.eval()
    device = next(model.parameters()).device

    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    generated = input_ids

    for _ in range(max_new_tokens):
        logits, _ = model(generated)
        next_token_logits = logits[:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        generated = torch.cat([generated, next_token], dim=1)

        # å¦‚æœç”Ÿæˆäº†ç»“æŸç¬¦ï¼Œå¯ä»¥æå‰åœæ­¢ï¼ˆå¯é€‰ï¼‰
        if next_token.item() == tokenizer.eos_token_id:
            break

    return tokenizer.decode(generated[0], skip_special_tokens=True)

# ========================
# 5. ä¸»ç¨‹åºå…¥å£
# ========================

if __name__ == "__main__":
    train()

    config = TinyGPTConfig()
    model = TinyGPT(config)
    checkpoint_data = torch.load("nanoGPT2_epoch_epoch_3.pt", map_location='cpu')
    model.load_state_dict(checkpoint_data['model_state_dict'])
    model.to('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    print(generate_text(model, tokenizer, "æ‰€è°“å…ƒå©´ä¹‹ä¸‹ç¬¬ä¸€äººï¼Œ"))